# CS-435-Big-data

#PA1 
An N-gram is a contiguous sequence of N items from a given sequence of text or speech. An N-gram
model is a type of probabilistic language model for predicting the next item in such a sequence in the
form of a (n-1). N-gram models are widely used in statistical natural language processing. In speech
recognition, phonemes and sequence of phonemes are modeled using a N-gram distribution. For
sequences of words, the 1-grams (aka unigram) generated from “We analyze large dataset” are (“We”,
“analyze”, “large”, “dataset”). For the same sentence, the 2-grams (aka bigram) are (“__ , We”, “We,
analyze”, “analyze, large”, “large, dataset”, “dataset, __”). Here, “__” represents the empty space
before and after the sentence.
N-grams are used for various applications such as approximate matching, plagiarism detection,
searching for the similar documents, automatic authorship detection, and linguistic cultural trend
analysis. Google’s Ngram Viewer 1 is a good example of N-gram analysis.
